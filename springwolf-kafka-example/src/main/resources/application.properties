import pandas as pd
import yaml
import numpy as np
import gh_methods

def load_config(config_path):
    with open(config_path, "r") as file:
        return yaml.safe_load(file)

def load_model_metadata(metadata_path):
    with open(metadata_path, "r") as file:
        return yaml.safe_load(file)

def get_cut_points(metadata):
    return metadata['cut_points']  # Aqui você ajusta conforme a estrutura do seu metadata

def process_inference_data(file_path, score_column, transformation_function, transformation_params):
    chunks = pd.read_csv(file_path, chunksize=1000)  # Ajuste o tamanho do chunk conforme necessário
    transformed_dfs = []

    for chunk in chunks:
        chunk[score_column] = transformation_function(chunk[score_column], **transformation_params)
        transformed_dfs.append(chunk)

    return pd.concat(transformed_dfs)

def save_transformed_data(df, output_path):
    df.to_csv(output_path, index=False)

def main():
    # Carregar metadados do modelo
    metadata = load_model_metadata("path/to/model_metadata.yml")
    
    # Carregar configurações do usuário
    config = load_config("path/to/config.yml")
    calculation_type = config["calculation_type"]
    
    # Obter pontos de corte para o cálculo
    cut_points = get_cut_points(metadata)
    
    # Preparar a função de transformação e parâmetros
    transformation_function, transformation_params = gh_methods.get_transformation_params(calculation_type, cut_points)
    
    # Processar os dados de inferência em chunks
    df = process_inference_data("path/to/inference_output.csv", "score", transformation_function, transformation_params)
    
    # Salvar os dados transformados
    save_transformed_data(df, "path/to/transformed_output.csv")

if __name__ == "__main__":
    main()
