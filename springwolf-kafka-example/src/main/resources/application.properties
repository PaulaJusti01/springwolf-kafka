import pandas as pd
import boto3
import yaml
from gh_methods import apply_transformation

def read_config(config_path):
    with open(config_path, 'r') as file:
        return yaml.safe_load(file)

def load_model_metadata(s3_bucket, metadata_key):
    s3_client = boto3.client('s3')
    obj = s3_client.get_object(Bucket=s3_bucket, Key=metadata_key)
    return obj['Body'].read()

def save_to_s3(df, s3_bucket, output_key):
    # Salva o DataFrame como CSV no S3
    csv_buffer = df.to_csv(index=False)
    s3_client = boto3.client('s3')
    s3_client.put_object(Bucket=s3_bucket, Key=output_key, Body=csv_buffer)

def process_data(s3_bucket, output_key, config_path):
    # 1) Carrega metadados do modelo em memória
    metadata = load_model_metadata(s3_bucket, "model/metadata.json")

    # 2) Lê o arquivo de output da inferência em memória usando chunks
    for chunk in pd.read_csv(f's3://{s3_bucket}/{output_key}', chunksize=1000):
        # 3) Executa a transformação
        # 3.1) Extrai a função de transformação do config.yml
        config = read_config(config_path)
        transformation_function = config.get("calculation")

        # 3.2) Aplica a transformação
        transformed_chunk = apply_transformation(chunk, "score", transformation_function)

        # 4) Salva o resultado no mesmo bucket mas em outro diretório
        save_to_s3(transformed_chunk, s3_bucket, "output/transformed_output.csv")

# Execução da função principal
if __name__ == "__main__":
    process_data("your-s3-bucket", "output/inference_output.csv", "config.yml")
