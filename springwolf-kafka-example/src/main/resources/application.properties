import json

def process_inference_output(args):
    calculation_type = args.calculation_type
    chunks = int(args.chunk)
    score_column = "score"

    for file_name in os.listdir(DEFAULT_INPUT_POS_DIR):
        if file_name.endswith('.parquet'):
            print(f"Inicializando o writer com o primeiro arquivo: {file_name}")
            file_path = os.path.join(DEFAULT_INPUT_POS_DIR, file_name)
            
            # Carrega todo o parquet na memória
            inference_df = pd.read_parquet(file_path)
            
            # Divide o DataFrame em chunks
            num_chunks = (len(inference_df) // chunks) + 1
            chunks = np.array_split(inference_df, num_chunks)

            for i, chunk in enumerate(chunks):
                # Transformação e salvamento do chunk
                transformed_chunk = apply_post_inference_calculation(
                    chunk, 
                    score_column=score_column, 
                    calculation_type=calculation_type
                )

                # Cria uma lista de dicionários com score e valor do cálculo
                transformed_chunk_json = [
                    {"score": row[score_column], "calculo_gh": row["calculo_gh"]}
                    for _, row in transformed_chunk.iterrows()
                ]
                
                transformed_chunk_key = f"{file_name}part_{i}.json"
                output_path = os.path.join(DEFAULT_OUTPUT_DIR, transformed_chunk_key)
                
                # Salva o JSON
                with open(output_path, 'w') as f:
                    json.dump(transformed_chunk_json, f, indent=4)
                
                print(f"Transformed chunk saved: {transformed_chunk_key}")
