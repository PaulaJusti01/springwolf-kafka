import os
import boto3
import yaml
import pandas as pd
from itau_mr7_drp_lotus_internals import apply_transformation  # Nome atualizado da biblioteca

# Inicializa clientes S3 e outras configurações
s3_client = boto3.client("s3")
s3_bucket_model_metadata = os.getenv("S3_BUCKET_MODEL_METADATA")
s3_bucket_inference_output = os.getenv("S3_BUCKET_INFERENCE_OUTPUT")
s3_bucket_transformed_output = os.getenv("S3_BUCKET_TRANSFORMED_OUTPUT")
config_path = os.getenv("CONFIG_PATH")

# Carrega os metadados do modelo em memória
def load_model_metadata():
    metadata_key = "model_metadata.yaml"  # Nome do arquivo no bucket S3
    metadata_object = s3_client.get_object(Bucket=s3_bucket_model_metadata, Key=metadata_key)
    metadata_content = metadata_object['Body'].read().decode('utf-8')
    metadata = yaml.safe_load(metadata_content)
    return metadata

# Carrega o parâmetro de cálculo do config.yml do usuário
def load_config():
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)
    calculation_type = config.get("calculation_type", None)
    if not calculation_type:
        raise ValueError("Parâmetro 'calculation_type' não encontrado no config.yml.")
    return calculation_type

# Processa o arquivo de inferência em chunks e aplica a transformação
def process_inference_output():
    # Carrega os metadados do modelo e o parâmetro de cálculo
    metadata = load_model_metadata()
    calculation_type = load_config()

    # Configurações para chunks
    inference_key = "inference_output.csv"
    transformed_key = "transformed_output/"
    chunk_size = 10000  # Ajuste conforme necessário

    # Realiza o download e processamento do arquivo de inferência
    inference_object = s3_client.get_object(Bucket=s3_bucket_inference_output, Key=inference_key)
    inference_df = pd.read_csv(inference_object['Body'], chunksize=chunk_size)

    for i, chunk in enumerate(inference_df):
        # Aplica a transformação usando a biblioteca
        score_column = "score"
        transformed_chunk = apply_transformation(
            chunk, 
            score_column=score_column, 
            calculation_type=calculation_type, 
            metadata=metadata
        )

        # Salva o chunk transformado em um novo diretório no bucket
        transformed_chunk_key = f"{transformed_key}part_{i}.csv"
        transformed_chunk_csv = transformed_chunk.to_csv(index=False)
        s3_client.put_object(
            Bucket=s3_bucket_transformed_output, 
            Key=transformed_chunk_key, 
            Body=transformed_chunk_csv
        )

if __name__ == "__main__":
    process_inference_output()
